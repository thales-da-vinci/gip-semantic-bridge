import { WebSocket } from 'ws';
import { v4 as uuidv4 } from 'uuid';
import chalk from 'chalk';
import axios from 'axios';

/**
 * GIP Semantic Bridge v0.1.0-alpha
 * Integrates Ollama LLMs with GIP Federation network
 * 
 * Architecture:
 * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 * â”‚ GIP Federation  â”‚â”€â”€â”€â–¶â”‚ Semantic Bridge  â”‚â”€â”€â”€â–¶â”‚ Ollama (LLM) â”‚
 * â”‚   (Port 8810)   â”‚    â”‚  (Port 8811)     â”‚    â”‚ (Port 11434) â”‚
 * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *         â–²                       â”‚
 *         â”‚                       â–¼
 *    Message Relay        Semantic Processing
 *         â”‚                       â”‚
 *         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *         Enriched Response Flow
 */

// ============================================================================
// Configuration
// ============================================================================

const FEDERATION_HOST = process.env.FEDERATION_HOST || 'localhost';
const FEDERATION_PORT = process.env.FEDERATION_PORT || 8810;
const BRIDGE_PORT = process.env.BRIDGE_PORT || 8811;
const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434';
const SEMANTIC_MODEL = process.env.SEMANTIC_MODEL || 'mistral';

// ============================================================================
// Types
// ============================================================================

interface SemanticRequest {
    id: string;
    prompt: string;
    model?: string;
    temperature?: number;
    topK?: number;
    topP?: number;
    timestamp: number;
}

interface SemanticResponse {
    id: string;
    prompt: string;
    model: string;
    response: string;
    tokens: {
        prompt: number;
        completion: number;
        total: number;
    };
    processingTime: number;
    timestamp: number;
}

interface OllamaMessage {
    role: 'user' | 'assistant' | 'system';
    content: string;
}

// ============================================================================
// Ollama Integration
// ============================================================================

class OllamaClient {
    private baseUrl: string;
    private model: string;
    private requestTimeout: number;

    constructor(baseUrl: string, model: string, timeout = 60000) {
        this.baseUrl = baseUrl;
        this.model = model;
        this.requestTimeout = timeout;
    }

    /**
     * Check if Ollama service is available
     */
    async isAvailable(): Promise<boolean> {
        try {
            const response = await axios.get(`${this.baseUrl}/api/tags`, {
                timeout: 5000,
            });
            return response.status === 200;
        } catch (error) {
            return false;
        }
    }

    /**
     * Get list of available models from Ollama
     */
    async listModels(): Promise<string[]> {
        try {
            const response = await axios.get(`${this.baseUrl}/api/tags`, {
                timeout: 5000,
            });
            return response.data.models?.map((m: any) => m.name) || [];
        } catch (error) {
            console.error(chalk.red(`âŒ Failed to list Ollama models: ${error}`));
            return [];
        }
    }

    /**
     * Generate text using Ollama
     */
    async generate(request: SemanticRequest): Promise<SemanticResponse> {
        const startTime = Date.now();

        try {
            console.log(
                chalk.blueBright(
                    `ğŸ§  Ollama: Processing "${request.prompt.substring(0, 50)}..."`
                )
            );

            const response = await axios.post(
                `${this.baseUrl}/api/generate`,
                {
                    model: request.model || this.model,
                    prompt: request.prompt,
                    temperature: request.temperature || 0.7,
                    top_k: request.topK || 40,
                    top_p: request.topP || 0.9,
                    stream: false,
                },
                { timeout: this.requestTimeout }
            );

            const processingTime = Date.now() - startTime;

            return {
                id: request.id,
                prompt: request.prompt,
                model: request.model || this.model,
                response: response.data.response,
                tokens: {
                    prompt: response.data.prompt_eval_count || 0,
                    completion: response.data.eval_count || 0,
                    total:
                        (response.data.prompt_eval_count || 0) +
                        (response.data.eval_count || 0),
                },
                processingTime,
                timestamp: Date.now(),
            };
        } catch (error: any) {
            console.error(
                chalk.red(
                    `âŒ Ollama generation failed: ${error.message || error}`
                )
            );
            throw error;
        }
    }

    /**
     * Chat mode (stateful conversation)
     */
    async chat(
        messages: OllamaMessage[],
        request: SemanticRequest
    ): Promise<SemanticResponse> {
        const startTime = Date.now();

        try {
            const response = await axios.post(
                `${this.baseUrl}/api/chat`,
                {
                    model: request.model || this.model,
                    messages,
                    temperature: request.temperature || 0.7,
                    stream: false,
                },
                { timeout: this.requestTimeout }
            );

            const processingTime = Date.now() - startTime;

            return {
                id: request.id,
                prompt: request.prompt,
                model: request.model || this.model,
                response: response.data.message.content,
                tokens: {
                    prompt: response.data.prompt_eval_count || 0,
                    completion: response.data.eval_count || 0,
                    total:
                        (response.data.prompt_eval_count || 0) +
                        (response.data.eval_count || 0),
                },
                processingTime,
                timestamp: Date.now(),
            };
        } catch (error: any) {
            console.error(
                chalk.red(
                    `âŒ Ollama chat failed: ${error.message || error}`
                )
            );
            throw error;
        }
    }
}

// ============================================================================
// Semantic Bridge Server
// ============================================================================

class SemanticBridge {
    private ollama: OllamaClient;
    private federationConnection: WebSocket | null = null;
    private messageCache: Map<string, SemanticResponse> = new Map();
    private processingQueue: SemanticRequest[] = [];

    constructor() {
        this.ollama = new OllamaClient(OLLAMA_URL, SEMANTIC_MODEL);
    }

    /**
     * Initialize bridge: connect to federation and verify Ollama
     */
    async initialize(): Promise<void> {
        console.log(chalk.cyanBright('ğŸŒ‰ Initializing Semantic Bridge...'));

        // Check Ollama availability
        const ollamaAvailable = await this.ollama.isAvailable();
        if (!ollamaAvailable) {
            console.warn(
                chalk.yellow(
                    `âš ï¸  Ollama not available at ${OLLAMA_URL}. Starting in offline mode.`
                )
            );
        } else {
            console.log(
                chalk.green(
                    `âœ… Ollama connected at ${OLLAMA_URL}`
                )
            );

            // List available models
            const models = await this.ollama.listModels();
            console.log(
                chalk.gray(
                    `ğŸ“¦ Available models: ${models.join(', ') || 'none'}`
                )
            );
        }

        // Connect to federation node
        await this.connectToFederation();
    }

    /**
     * Connect to GIP Federation relay
     */
    private connectToFederation(): Promise<void> {
        return new Promise((resolve, reject) => {
            try {
                const url = `ws://${FEDERATION_HOST}:${FEDERATION_PORT}`;
                console.log(chalk.gray(`ğŸ”— Connecting to federation: ${url}`));

                this.federationConnection = new WebSocket(url);

                this.federationConnection.on('open', () => {
                    console.log(
                        chalk.green(
                            `âœ… Connected to GIP Federation relay`
                        )
                    );
                    resolve();
                });

                this.federationConnection.on('message', (data: Buffer) => {
                    this.handleFederationMessage(data.toString());
                });

                this.federationConnection.on('error', (error) => {
                    console.error(
                        chalk.red(
                            `âŒ Federation connection error: ${error.message}`
                        )
                    );
                    reject(error);
                });

                this.federationConnection.on('close', () => {
                    console.warn(
                        chalk.yellow(
                            `âš ï¸  Disconnected from federation (will reconnect...)`
                        )
                    );
                    setTimeout(() => this.connectToFederation(), 5000);
                });
            } catch (error) {
                reject(error);
            }
        });
    }

    /**
     * Handle incoming messages from federation
     */
    private async handleFederationMessage(message: string): Promise<void> {
        try {
            const data = JSON.parse(message);

            if (!data.type) {
                console.warn(chalk.yellow(`âš ï¸  Invalid message: missing type`));
                return;
            }

            if (data.type === 'semantic_request') {
                const request: SemanticRequest = {
                    id: data.id || uuidv4(),
                    prompt: data.prompt,
                    model: data.model,
                    temperature: data.temperature,
                    topK: data.topK,
                    topP: data.topP,
                    timestamp: Date.now(),
                };

                this.processingQueue.push(request);
                await this.processRequest(request);
            }
        } catch (error) {
            console.error(
                chalk.red(
                    `âŒ Failed to handle federation message: ${error}`
                )
            );
        }
    }

    /**
     * Process semantic request through Ollama
     */
    private async processRequest(request: SemanticRequest): Promise<void> {
        try {
            const response = await this.ollama.generate(request);

            // Cache result
            this.messageCache.set(request.id, response);

            // Log result
            console.log(
                chalk.green(
                    `âœ… [${request.id}] Generated ${response.tokens.completion} tokens in ${response.processingTime}ms`
                )
            );

            // Send response back through federation
            if (this.federationConnection?.readyState === 1) {
                this.federationConnection.send(
                    JSON.stringify({
                        type: 'semantic_response',
                        ...response,
                    })
                );
            }
        } catch (error) {
            console.error(chalk.red(`âŒ Request processing failed: ${error}`));

            // Send error response
            if (this.federationConnection?.readyState === 1) {
                this.federationConnection.send(
                    JSON.stringify({
                        type: 'semantic_error',
                        id: request.id,
                        error: String(error),
                        timestamp: Date.now(),
                    })
                );
            }
        }
    }

    /**
     * Get semantic response from cache
     */
    getResponse(id: string): SemanticResponse | undefined {
        return this.messageCache.get(id);
    }

    /**
     * Get processing statistics
     */
    getStats(): {
        queueLength: number;
        cacheSize: number;
        cachedResponses: string[];
    } {
        return {
            queueLength: this.processingQueue.length,
            cacheSize: this.messageCache.size,
            cachedResponses: Array.from(this.messageCache.keys()),
        };
    }
}

// ============================================================================
// Main Execution
// ============================================================================

async function main() {
    try {
        const bridge = new SemanticBridge();
        await bridge.initialize();

        console.log(
            chalk.cyanBright(
                `\nğŸŒ‰ GIP Semantic Bridge started on port ${BRIDGE_PORT}`
            )
        );
        console.log(
            chalk.gray(
                `   Federation: ws://${FEDERATION_HOST}:${FEDERATION_PORT}`
            )
        );
        console.log(
            chalk.gray(
                `   Ollama: ${OLLAMA_URL}`
            )
        );
        console.log(chalk.gray(`   Model: ${SEMANTIC_MODEL}\n`));

        // Keep process alive
        setInterval(() => {
            const stats = bridge.getStats();
            if (stats.queueLength > 0 || stats.cacheSize > 0) {
                console.log(
                    chalk.gray(
                        `ğŸ“Š Queue: ${stats.queueLength}, Cache: ${stats.cacheSize}`
                    )
                );
            }
        }, 30000);
    } catch (error) {
        console.error(
            chalk.red(
                `âŒ Fatal error: ${error}`
            )
        );
        process.exit(1);
    }
}

main();
