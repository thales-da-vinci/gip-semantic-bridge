# GIP Semantic Bridge

Semantic intelligence bridge for GIP Federation network. Integrates Ollama LLMs (Mistral, Phi, Gemma) with the GIP Federation relay to enable semantic reasoning across distributed nodes.

## Features

- ğŸ§  **Ollama Integration**: Direct API bridge to local LLM models
- ğŸŒ **Federation Connected**: WebSocket relay with GIP Federation node
- ğŸ“¨ **Message Routing**: Semantic requests/responses with request ID tracking
- âš¡ **Caching**: Response caching to reduce redundant LLM calls
- ğŸ“Š **Statistics**: Real-time processing queue and cache metrics
- ğŸ³ **Containerized**: Docker + docker-compose for orchestration
- ğŸ§ª **Tested**: Jest test suite (10 tests covering integration points)

## Installation

### Local Development

```bash
npm install
npm run build
npm run dev
```

### Docker

```bash
docker-compose up -d
```

## Configuration

Environment variables:

```env
FEDERATION_HOST=localhost      # GIP Federation relay host
FEDERATION_PORT=8810           # GIP Federation relay port
BRIDGE_PORT=8811               # Semantic Bridge port
OLLAMA_URL=http://localhost:11434  # Ollama service URL
SEMANTIC_MODEL=mistral         # Default LLM model (mistral, phi, gemma)
```

## Usage

### Message Format

**Semantic Request** (from federation):
```json
{
  "type": "semantic_request",
  "id": "msg-123",
  "prompt": "What is quantum computing?",
  "model": "mistral",
  "temperature": 0.7,
  "topK": 40,
  "topP": 0.9
}
```

**Semantic Response** (back to federation):
```json
{
  "type": "semantic_response",
  "id": "msg-123",
  "prompt": "What is quantum computing?",
  "model": "mistral",
  "response": "Quantum computing is...",
  "tokens": {
    "prompt": 5,
    "completion": 142,
    "total": 147
  },
  "processingTime": 2340,
  "timestamp": 1705337700000
}
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GIP Federation â”‚
â”‚  Relay Node     â”‚
â”‚  (Port 8810)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ WebSocket
         â”‚ Semantic Request
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Semantic Bridge       â”‚
â”‚ (Port 8811)           â”‚
â”‚                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Message Handler  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚          â”‚           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Ollama Client    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚          â”‚           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Response Cache   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ HTTP
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Engine   â”‚
â”‚ (Port 11434)    â”‚
â”‚                 â”‚
â”‚ Models:         â”‚
â”‚ - Mistral 7B    â”‚
â”‚ - Phi-2         â”‚
â”‚ - Gemma 7B      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Testing

```bash
npm test
npm test -- --coverage  # With coverage report
npm run test:watch      # Watch mode
```

### Test Suite (10 tests)

1. âœ… Bridge initialization
2. âœ… Ollama model availability detection
3. âœ… Semantic request parsing
4. âœ… Response caching
5. âœ… Federation message validation
6. âœ… Invalid request handling
7. âœ… Statistics aggregation
8. âœ… Token counting
9. âœ… Processing time measurement
10. âœ… Temperature parameter validation

## API

### SemanticBridge Class

#### `initialize(): Promise<void>`
Initialize bridge, verify Ollama, and connect to federation.

#### `getResponse(id: string): SemanticResponse | undefined`
Retrieve cached semantic response by request ID.

#### `getStats(): Statistics`
Get real-time processing statistics.

### OllamaClient Class

#### `isAvailable(): Promise<boolean>`
Check if Ollama service is running.

#### `listModels(): Promise<string[]>`
Get list of available models in Ollama.

#### `generate(request: SemanticRequest): Promise<SemanticResponse>`
Generate text using specified model.

#### `chat(messages: OllamaMessage[], request: SemanticRequest): Promise<SemanticResponse>`
Chat mode with conversation history.

## Supported Models

Default: **Mistral 7B**

Other models (via `model` parameter):
- **Phi-2** (Microsoft)
- **Gemma 7B** (Google)
- **Llama 2** (Meta)
- **Neural Chat**
- **Zephyr**

## Logs

Streaming JSON logs to stdout:
```
[INFO] ğŸŒ‰ Initializing Semantic Bridge...
[INFO] âœ… Ollama connected at http://localhost:11434
[INFO] ğŸ“¦ Available models: mistral, phi, gemma
[INFO] ğŸ”— Connecting to federation: ws://localhost:8810
[INFO] âœ… Connected to GIP Federation relay
[INFO] ğŸŒ‰ GIP Semantic Bridge started on port 8811
[DEBUG] ğŸ§  Ollama: Processing "What is quantum computing..."
[INFO] âœ… [msg-123] Generated 142 tokens in 2340ms
```

## Performance

- **Latency**: 2-5s (depends on model size and hardware)
- **Throughput**: ~1-2 requests/sec per bridge instance
- **Memory**: ~2-4GB (depends on loaded model)
- **Cache Hit Rate**: 70%+ for repeated queries

## Troubleshooting

### "Ollama not available"
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama if not running
ollama serve
```

### "Cannot connect to federation"
```bash
# Check if federation node is running
curl http://localhost:8810

# Start federation in separate terminal
cd gip-federation
npm run dev
```

### "Module not found" errors
```bash
npm install
npm run build
```

## Related Modules

- [gip-federation](https://github.com/Tehkne-Solutions/gip-federation) - WebSocket relay
- [gip-core](https://github.com/Tehkne-Solutions/gip-core) - Core architecture
- [gip-symphonia](https://github.com/Tehkne-Solutions/gip-symphonia) - Cognitive engine

## Version History

- **0.1.0-alpha** (2025-01-15)
  - Initial release
  - Ollama integration
  - Federation connectivity
  - Jest test suite (10/10 passing)
  - Docker containerization

## License

MIT Â© TehknÃ© Solutions

---

Built with â¤ï¸ for distributed cognitive systems.
